{
    "metadata": {
        "kernelspec": {
            "name": "SQL",
            "display_name": "SQL",
            "language": "sql"
        },
        "language_info": {
            "name": "sql",
            "version": ""
        },
        "application/vnd.databricks.v1+notebook": {
            "dashboards": [],
            "environmentMetadata": null,
            "language": "python",
            "notebookMetadata": {
                "pythonIndentUnit": 2
            },
            "notebookName": "0-mount-database-setup",
            "widgets": {}
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "\n",
                "### Step 1: Connect to ADLS\n",
                "\n",
                "Since we are using Azure Credentials Passthrough connection method, we just need to use the custom access token and mount the folder that has all our TestStand files. "
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "84d079e4-93e8-460d-9d95-54a3fa326ca1",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "666709a3-8618-48ea-811e-6c04943fa881"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Install fsspec if needed\n",
                "# %pip install fsspec\n",
                "\n",
                "# Declare script variables\n",
                "database_name = \"dbo\"\n",
                "\n",
                "bronze_source = \"REDACTED\"\n",
                "bronze_target = bronze_source + database_name\n",
                "bronze_mnt = \"/mnt/bronze/\" + database_name\n",
                "\n",
                "silver_source = \"REDACTED\"\n",
                "silver_target = silver_source + database_name\n",
                "silver_mnt = \"/mnt/silver/\" + database_name\n",
                "\n",
                "gold_source = \"REDACTED\"\n",
                "gold_target = gold_source + database_name\n",
                "gold_mnt = \"/mnt/gold/\" + database_name\n",
                "\n",
                "# Azure Credentials Passthrough configuration.\n",
                "configs = {\n",
                "  \"fs.azure.account.auth.type\": \"CustomAccessToken\",\n",
                "  \"fs.azure.account.custom.token.provider.class\": spark.conf.get(\"spark.databricks.passthrough.adls.gen2.tokenProviderClassName\")\n",
                "}\n",
                ""
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "c760fc54-dbf0-4715-8df6-8fdff699c029",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "83df85a1-ad63-418c-9efb-066c2d4ef54a",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "\n",
                "### Step 2: Mount bronze container\n"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "51114086-25f5-4c9c-8bb3-64ff28b0f0f4",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "14296369-9797-47e8-a94f-315f6b3fdefa"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Add bronze_mnt if not already mounted.\n",
                "if any(mount.mountPoint == bronze_mnt for mount in dbutils.fs.mounts()):\n",
                "    pass\n",
                "else:\n",
                "    dbutils.fs.mount(\n",
                "        source=bronze_source, mount_point=bronze_mnt, extra_configs=configs\n",
                "    )\n",
                "\n",
                "# List files in directory\n",
                "dbutils.fs.ls(bronze_target)"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "f6366891-7da1-478e-8094-4291f4fca976",
                    "showTitle": false,
                    "title": ""
                },
                "jupyter": {
                    "outputs_hidden": true
                },
                "azdata_cell_guid": "8cc9dd8b-9fb6-40c6-9c4c-ecd51291cc8e",
                "language": "sql"
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 3,
                    "data": {
                        "text/plain": "[FileInfo(path='abfss://bronze@arcdatalakeg2.dfs.core.windows.net/dbo/PROP_ANALOGWAVEFORM.parquet', name='PROP_ANALOGWAVEFORM.parquet', size=343, modificationTime=1708270317000),\n FileInfo(path='abfss://bronze@arcdatalakeg2.dfs.core.windows.net/dbo/PROP_BINARY.parquet', name='PROP_BINARY.parquet', size=1044738, modificationTime=1708270299000),\n FileInfo(path='abfss://bronze@arcdatalakeg2.dfs.core.windows.net/dbo/PROP_DIGITALWAVEFORM.parquet', name='PROP_DIGITALWAVEFORM.parquet', size=311, modificationTime=1708270284000),\n FileInfo(path='abfss://bronze@arcdatalakeg2.dfs.core.windows.net/dbo/PROP_NUMERICLIMIT.parquet', name='PROP_NUMERICLIMIT.parquet', size=1289983, modificationTime=1708270268000),\n FileInfo(path='abfss://bronze@arcdatalakeg2.dfs.core.windows.net/dbo/PROP_RESULT.parquet', name='PROP_RESULT.parquet', size=23944627, modificationTime=1708270257000),\n FileInfo(path='abfss://bronze@arcdatalakeg2.dfs.core.windows.net/dbo/STEP_RESULT.parquet', name='STEP_RESULT.parquet', size=17115510, modificationTime=1708270237000),\n FileInfo(path='abfss://bronze@arcdatalakeg2.dfs.core.windows.net/dbo/STEP_SEQCALL.parquet', name='STEP_SEQCALL.parquet', size=732123, modificationTime=1708270216000),\n FileInfo(path='abfss://bronze@arcdatalakeg2.dfs.core.windows.net/dbo/UUT_RESULT.parquet', name='UUT_RESULT.parquet', size=91031, modificationTime=1708270197000)]"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "\n",
                "### Step 3: Mount silver container\n"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "92108b48-e8f5-4d0d-9d24-3775b36ca207",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "9dc5e030-e7a2-4aa0-95fc-fb83d0b7c4e7"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Add silver_mnt if not already mounted.\n",
                "if any(mount.mountPoint == silver_mnt for mount in dbutils.fs.mounts()):\n",
                "    pass\n",
                "else:\n",
                "    dbutils.fs.mount(\n",
                "        source=silver_source, mount_point=silver_mnt, extra_configs=configs\n",
                "    )\n",
                "\n",
                "# List files in directory\n",
                "dbutils.fs.ls(silver_target)"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "fec0eb47-1220-48a6-81b4-adb261a7265a",
                    "showTitle": false,
                    "title": ""
                },
                "jupyter": {
                    "outputs_hidden": true
                },
                "azdata_cell_guid": "ef371717-7aa9-407e-83fa-ecc403ed0547",
                "language": "sql"
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 4,
                    "data": {
                        "text/plain": "[]"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "\n",
                "### Step 4: Mount gold container\n"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "8abe7239-0632-4b90-baf0-38549b038a38",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "1497649e-24be-438a-97a1-964dfcfe66f0"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Add gold_mnt if not already mounted.\n",
                "if any(mount.mountPoint == gold_mnt for mount in dbutils.fs.mounts()):\n",
                "    pass\n",
                "else:\n",
                "    dbutils.fs.mount(\n",
                "      source=gold_source, mount_point=gold_mnt, extra_configs=configs\n",
                "    )\n",
                "\n",
                "# List files in directory\n",
                "dbutils.fs.ls(gold_target)"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
                    "showTitle": false,
                    "title": ""
                },
                "jupyter": {
                    "outputs_hidden": true
                },
                "azdata_cell_guid": "d70fef22-07f0-4e88-aeb1-1f3a9cd7ccfe",
                "language": "sql"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mExecutionError\u001b[0m                            Traceback (most recent call last)\nFile \u001b[0;32m<command-2461738334509896>, line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m     dbutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mmount(\n\u001b[1;32m      6\u001b[0m         source\u001b[38;5;241m=\u001b[39mbronze_source, mount_point\u001b[38;5;241m=\u001b[39mbronze_mnt, extra_configs\u001b[38;5;241m=\u001b[39mconfigs\n\u001b[1;32m      7\u001b[0m     )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# List files in directory\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m dbutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mls(bronze_target)\n\nFile \u001b[0;32m/databricks/python_shell/dbruntime/dbutils.py:378\u001b[0m, in \u001b[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m exc\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m exc\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n\n\u001b[0;31mExecutionError\u001b[0m: An error occurred while calling o406.ls.\n: Failure to initialize configuration for storage account arcdatalakeg2.dfs.core.windows.net: Invalid configuration value detected for fs.azure.account.keyInvalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:52)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getStorageAccountKey(AbfsConfiguration.java:682)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:2076)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:268)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:235)\n\tat com.databricks.common.filesystem.LokiABFS.initialize(LokiABFS.scala:36)\n\tat com.databricks.common.filesystem.LokiFileSystem$.$anonfun$getLokiFS$1(LokiFileSystem.scala:157)\n\tat com.databricks.common.filesystem.FileSystemCache.getOrCompute(FileSystemCache.scala:46)\n\tat com.databricks.common.filesystem.LokiFileSystem$.getLokiFS(LokiFileSystem.scala:154)\n\tat com.databricks.common.filesystem.LokiFileSystem.initialize(LokiFileSystem.scala:219)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3611)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:554)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:215)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:208)\n\tat com.databricks.unity.FallbackToClusterDefaultSAM.createDelegate(SAM.scala:252)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:85)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:137)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.getReadDelegate(CredentialScopeFileSystem.scala:98)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.listStatus(CredentialScopeFileSystem.scala:233)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:295)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$4(DBUtilsCore.scala:264)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:151)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$3(DBUtilsCore.scala:264)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:146)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:263)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:236)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:582)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:685)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:703)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:435)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:433)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:427)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:73)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:481)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:464)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:73)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:680)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:591)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:73)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:582)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:551)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:73)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:137)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:236)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: Invalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.ConfigurationBasicValidator.validate(ConfigurationBasicValidator.java:49)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.Base64StringConfigurationBasicValidator.validate(Base64StringConfigurationBasicValidator.java:40)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.validateStorageAccountKey(SimpleKeyProvider.java:71)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:49)\n\t... 59 more\n"
                    },
                    "metadata": {
                        "application/vnd.databricks.v1+output": {
                            "addedWidgets": {},
                            "arguments": {},
                            "datasetInfos": [],
                            "jupyterProps": {
                                "ename": "ExecutionError",
                                "evalue": "An error occurred while calling o406.ls.\n: Failure to initialize configuration for storage account arcdatalakeg2.dfs.core.windows.net: Invalid configuration value detected for fs.azure.account.keyInvalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:52)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getStorageAccountKey(AbfsConfiguration.java:682)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:2076)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:268)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:235)\n\tat com.databricks.common.filesystem.LokiABFS.initialize(LokiABFS.scala:36)\n\tat com.databricks.common.filesystem.LokiFileSystem$.$anonfun$getLokiFS$1(LokiFileSystem.scala:157)\n\tat com.databricks.common.filesystem.FileSystemCache.getOrCompute(FileSystemCache.scala:46)\n\tat com.databricks.common.filesystem.LokiFileSystem$.getLokiFS(LokiFileSystem.scala:154)\n\tat com.databricks.common.filesystem.LokiFileSystem.initialize(LokiFileSystem.scala:219)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3611)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:554)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:215)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:208)\n\tat com.databricks.unity.FallbackToClusterDefaultSAM.createDelegate(SAM.scala:252)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:85)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:137)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.getReadDelegate(CredentialScopeFileSystem.scala:98)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.listStatus(CredentialScopeFileSystem.scala:233)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:295)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$4(DBUtilsCore.scala:264)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:151)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$3(DBUtilsCore.scala:264)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:146)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:263)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:236)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:582)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:685)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:703)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:435)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:433)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:427)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:73)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:481)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:464)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:73)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:680)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:591)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:73)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:582)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:551)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:73)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:137)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:236)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: Invalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.ConfigurationBasicValidator.validate(ConfigurationBasicValidator.java:49)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.Base64StringConfigurationBasicValidator.validate(Base64StringConfigurationBasicValidator.java:40)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.validateStorageAccountKey(SimpleKeyProvider.java:71)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:49)\n\t... 59 more\n"
                            },
                            "metadata": {
                                "errorSummary": "Command skipped"
                            },
                            "removedWidgets": [],
                            "sqlProps": null,
                            "stackFrames": [
                                "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                                "\u001b[0;31mExecutionError\u001b[0m                            Traceback (most recent call last)",
                                "File \u001b[0;32m<command-2461738334509896>, line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m     dbutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mmount(\n\u001b[1;32m      6\u001b[0m         source\u001b[38;5;241m=\u001b[39mbronze_source, mount_point\u001b[38;5;241m=\u001b[39mbronze_mnt, extra_configs\u001b[38;5;241m=\u001b[39mconfigs\n\u001b[1;32m      7\u001b[0m     )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# List files in directory\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m dbutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mls(bronze_target)\n",
                                "File \u001b[0;32m/databricks/python_shell/dbruntime/dbutils.py:378\u001b[0m, in \u001b[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m exc\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m exc\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
                                "\u001b[0;31mExecutionError\u001b[0m: An error occurred while calling o406.ls.\n: Failure to initialize configuration for storage account arcdatalakeg2.dfs.core.windows.net: Invalid configuration value detected for fs.azure.account.keyInvalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:52)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getStorageAccountKey(AbfsConfiguration.java:682)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:2076)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:268)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:235)\n\tat com.databricks.common.filesystem.LokiABFS.initialize(LokiABFS.scala:36)\n\tat com.databricks.common.filesystem.LokiFileSystem$.$anonfun$getLokiFS$1(LokiFileSystem.scala:157)\n\tat com.databricks.common.filesystem.FileSystemCache.getOrCompute(FileSystemCache.scala:46)\n\tat com.databricks.common.filesystem.LokiFileSystem$.getLokiFS(LokiFileSystem.scala:154)\n\tat com.databricks.common.filesystem.LokiFileSystem.initialize(LokiFileSystem.scala:219)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3611)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:554)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:215)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:208)\n\tat com.databricks.unity.FallbackToClusterDefaultSAM.createDelegate(SAM.scala:252)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:85)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:137)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.getReadDelegate(CredentialScopeFileSystem.scala:98)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.listStatus(CredentialScopeFileSystem.scala:233)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:295)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$4(DBUtilsCore.scala:264)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:151)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$3(DBUtilsCore.scala:264)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:146)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:263)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:236)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:582)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:685)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:703)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:435)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:433)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:427)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:73)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:481)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:464)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:73)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:680)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:591)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:73)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:582)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:551)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:73)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:137)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:236)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: Invalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.ConfigurationBasicValidator.validate(ConfigurationBasicValidator.java:49)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.Base64StringConfigurationBasicValidator.validate(Base64StringConfigurationBasicValidator.java:40)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.validateStorageAccountKey(SimpleKeyProvider.java:71)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:49)\n\t... 59 more\n"
                            ],
                            "type": "baseError"
                        }
                    }
                }
            ],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "\n",
                "### Step 5: Setup Database and Extract Data to Tables in Databricks\n",
                "\n",
                "We instantiate a DBFS database and USE it. Then we iterate over the TSDB directory at that level and create a table for each CSV file in that mount point. \n"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "924e86aa-daeb-4954-beaf-9af51c50547a",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "9cbe2019-ae6a-4075-9c5c-02ccc094426b"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Create and use DATABASE [drop IF ALREADY EXISTS]\n",
                "spark.sql(f\"drop database IF EXISTS {database_name} cascade \")\n",
                "spark.sql(f\"create database {database_name}\")\n",
                "spark.sql(f\"use {database_name}\")\n",
                "\n",
                "# Create all tables into DBFS database\n",
                "tables_list = dbutils.fs.ls(bronze_target)\n",
                "\n",
                "for table in tables_list:\n",
                "    table_name = table.name.strip(\".parquet\")\n",
                "    spark.sql(f\"DROP TABLE IF EXISTS `{table_name}`\")\n",
                "    spark.sql(f\"\"\"\n",
                "              CREATE TABLE `{table_name}` \n",
                "              USING parquet \n",
                "              OPTIONS(path '{table.path}', header 'true', inferschema 'true')\n",
                "              \"\"\")\n",
                "    \n",
                "# Display table\n",
                "if debug_mode:\n",
                "    df = spark.sql('select * from uut_result')\n",
                "    display(df)"
            ],
            "metadata": {
                "application/vnd.databricks.v1+cell": {
                    "cellMetadata": {
                        "byteLimit": 2048000,
                        "rowLimit": 10000
                    },
                    "inputWidgets": {},
                    "nuid": "b799691d-119e-4451-af1e-ecec87e6083b",
                    "showTitle": false,
                    "title": ""
                },
                "azdata_cell_guid": "e1a04665-9c8d-45a8-95ff-8cae824b1dea",
                "language": "sql"
            },
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mExecutionError\u001b[0m                            Traceback (most recent call last)\nFile \u001b[0;32m<command-2461738334509896>, line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m     dbutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mmount(\n\u001b[1;32m      6\u001b[0m         source\u001b[38;5;241m=\u001b[39mbronze_source, mount_point\u001b[38;5;241m=\u001b[39mbronze_mnt, extra_configs\u001b[38;5;241m=\u001b[39mconfigs\n\u001b[1;32m      7\u001b[0m     )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# List files in directory\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m dbutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mls(bronze_target)\n\nFile \u001b[0;32m/databricks/python_shell/dbruntime/dbutils.py:378\u001b[0m, in \u001b[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m exc\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m exc\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n\n\u001b[0;31mExecutionError\u001b[0m: An error occurred while calling o406.ls.\n: Failure to initialize configuration for storage account arcdatalakeg2.dfs.core.windows.net: Invalid configuration value detected for fs.azure.account.keyInvalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:52)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getStorageAccountKey(AbfsConfiguration.java:682)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:2076)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:268)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:235)\n\tat com.databricks.common.filesystem.LokiABFS.initialize(LokiABFS.scala:36)\n\tat com.databricks.common.filesystem.LokiFileSystem$.$anonfun$getLokiFS$1(LokiFileSystem.scala:157)\n\tat com.databricks.common.filesystem.FileSystemCache.getOrCompute(FileSystemCache.scala:46)\n\tat com.databricks.common.filesystem.LokiFileSystem$.getLokiFS(LokiFileSystem.scala:154)\n\tat com.databricks.common.filesystem.LokiFileSystem.initialize(LokiFileSystem.scala:219)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3611)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:554)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:215)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:208)\n\tat com.databricks.unity.FallbackToClusterDefaultSAM.createDelegate(SAM.scala:252)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:85)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:137)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.getReadDelegate(CredentialScopeFileSystem.scala:98)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.listStatus(CredentialScopeFileSystem.scala:233)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:295)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$4(DBUtilsCore.scala:264)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:151)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$3(DBUtilsCore.scala:264)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:146)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:263)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:236)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:582)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:685)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:703)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:435)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:433)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:427)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:73)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:481)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:464)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:73)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:680)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:591)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:73)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:582)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:551)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:73)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:137)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:236)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: Invalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.ConfigurationBasicValidator.validate(ConfigurationBasicValidator.java:49)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.Base64StringConfigurationBasicValidator.validate(Base64StringConfigurationBasicValidator.java:40)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.validateStorageAccountKey(SimpleKeyProvider.java:71)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:49)\n\t... 59 more\n"
                    },
                    "metadata": {
                        "application/vnd.databricks.v1+output": {
                            "addedWidgets": {},
                            "arguments": {},
                            "datasetInfos": [],
                            "jupyterProps": {
                                "ename": "ExecutionError",
                                "evalue": "An error occurred while calling o406.ls.\n: Failure to initialize configuration for storage account arcdatalakeg2.dfs.core.windows.net: Invalid configuration value detected for fs.azure.account.keyInvalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:52)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getStorageAccountKey(AbfsConfiguration.java:682)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:2076)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:268)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:235)\n\tat com.databricks.common.filesystem.LokiABFS.initialize(LokiABFS.scala:36)\n\tat com.databricks.common.filesystem.LokiFileSystem$.$anonfun$getLokiFS$1(LokiFileSystem.scala:157)\n\tat com.databricks.common.filesystem.FileSystemCache.getOrCompute(FileSystemCache.scala:46)\n\tat com.databricks.common.filesystem.LokiFileSystem$.getLokiFS(LokiFileSystem.scala:154)\n\tat com.databricks.common.filesystem.LokiFileSystem.initialize(LokiFileSystem.scala:219)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3611)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:554)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:215)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:208)\n\tat com.databricks.unity.FallbackToClusterDefaultSAM.createDelegate(SAM.scala:252)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:85)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:137)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.getReadDelegate(CredentialScopeFileSystem.scala:98)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.listStatus(CredentialScopeFileSystem.scala:233)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:295)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$4(DBUtilsCore.scala:264)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:151)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$3(DBUtilsCore.scala:264)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:146)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:263)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:236)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:582)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:685)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:703)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:435)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:433)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:427)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:73)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:481)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:464)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:73)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:680)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:591)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:73)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:582)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:551)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:73)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:137)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:236)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: Invalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.ConfigurationBasicValidator.validate(ConfigurationBasicValidator.java:49)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.Base64StringConfigurationBasicValidator.validate(Base64StringConfigurationBasicValidator.java:40)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.validateStorageAccountKey(SimpleKeyProvider.java:71)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:49)\n\t... 59 more\n"
                            },
                            "metadata": {
                                "errorSummary": "Command skipped"
                            },
                            "removedWidgets": [],
                            "sqlProps": null,
                            "stackFrames": [
                                "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                                "\u001b[0;31mExecutionError\u001b[0m                            Traceback (most recent call last)",
                                "File \u001b[0;32m<command-2461738334509896>, line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m     dbutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mmount(\n\u001b[1;32m      6\u001b[0m         source\u001b[38;5;241m=\u001b[39mbronze_source, mount_point\u001b[38;5;241m=\u001b[39mbronze_mnt, extra_configs\u001b[38;5;241m=\u001b[39mconfigs\n\u001b[1;32m      7\u001b[0m     )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# List files in directory\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m dbutils\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mls(bronze_target)\n",
                                "File \u001b[0;32m/databricks/python_shell/dbruntime/dbutils.py:378\u001b[0m, in \u001b[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m exc\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m exc\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
                                "\u001b[0;31mExecutionError\u001b[0m: An error occurred while calling o406.ls.\n: Failure to initialize configuration for storage account arcdatalakeg2.dfs.core.windows.net: Invalid configuration value detected for fs.azure.account.keyInvalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:52)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AbfsConfiguration.getStorageAccountKey(AbfsConfiguration.java:682)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.initializeClient(AzureBlobFileSystemStore.java:2076)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.<init>(AzureBlobFileSystemStore.java:268)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.initialize(AzureBlobFileSystem.java:235)\n\tat com.databricks.common.filesystem.LokiABFS.initialize(LokiABFS.scala:36)\n\tat com.databricks.common.filesystem.LokiFileSystem$.$anonfun$getLokiFS$1(LokiFileSystem.scala:157)\n\tat com.databricks.common.filesystem.FileSystemCache.getOrCompute(FileSystemCache.scala:46)\n\tat com.databricks.common.filesystem.LokiFileSystem$.getLokiFS(LokiFileSystem.scala:154)\n\tat com.databricks.common.filesystem.LokiFileSystem.initialize(LokiFileSystem.scala:219)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3611)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:554)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat com.databricks.unity.SAM.createDelegate(SAM.scala:215)\n\tat com.databricks.unity.SAM.createDelegate$(SAM.scala:208)\n\tat com.databricks.unity.FallbackToClusterDefaultSAM.createDelegate(SAM.scala:252)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.createDelegate(CredentialScopeFileSystem.scala:85)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.$anonfun$setDelegates$1(CredentialScopeFileSystem.scala:137)\n\tat com.databricks.sql.acl.fs.Lazy.apply(DelegatingFileSystem.scala:310)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.getReadDelegate(CredentialScopeFileSystem.scala:98)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.listStatus(CredentialScopeFileSystem.scala:233)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:295)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$4(DBUtilsCore.scala:264)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:151)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$3(DBUtilsCore.scala:264)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:146)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:263)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:236)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:582)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:685)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:703)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:435)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:433)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:427)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:73)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:481)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:464)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:73)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:680)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:591)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:73)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:582)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:551)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:73)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:137)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:236)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: Invalid configuration value detected for fs.azure.account.key\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.ConfigurationBasicValidator.validate(ConfigurationBasicValidator.java:49)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.diagnostics.Base64StringConfigurationBasicValidator.validate(Base64StringConfigurationBasicValidator.java:40)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.validateStorageAccountKey(SimpleKeyProvider.java:71)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:49)\n\t... 59 more\n"
                            ],
                            "type": "baseError"
                        }
                    }
                }
            ],
            "execution_count": null
        }
    ]
}